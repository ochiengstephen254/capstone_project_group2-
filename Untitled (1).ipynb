{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59393b34-a7e2-4be3-a7e9-f32985db193e",
   "metadata": {},
   "source": [
    "# MEDIBOT - SOCIAL HEALTH INSURANCE CHATBOT PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808012b-c3ec-4d0e-bbd2-b367bf59b9b6",
   "metadata": {},
   "source": [
    "Authors : Asam Olala, Immaculate Kithei, Derrick Waititu, Stephen Ochieng, Elizabeth Atieno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf4c78-4910-4f2f-a637-4c298fc96e85",
   "metadata": {},
   "source": [
    "### Table of Contents \n",
    "\n",
    "[Data Exploration](#Data-Exploration)\n",
    "\n",
    "[Data Preprocessing](#Data-Preprocessing)\n",
    "\n",
    "\n",
    "[Modelling](#Modelling)\n",
    "\n",
    "-[XG Boost](#XG-Boost)\n",
    "\n",
    "-[Bert Model](#Bert-Model)\n",
    "\n",
    "-[Rasa Model](#Rasa-Model)\n",
    "\n",
    "[Model Comparison](#Model-Comparison)\n",
    "\n",
    "[Conclusion](Conclusion)\n",
    "\n",
    "[Recommendation](#Recommendation)\n",
    "\n",
    "[Next Steps](#Next-Steps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0546fc2-3bfb-46a2-a373-5e40a757a2f3",
   "metadata": {},
   "source": [
    " ### 1.Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45527eb7-912e-4a2e-95d6-4ffdb7e3c713",
   "metadata": {},
   "source": [
    "#### Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc19b6-9bfc-4b95-aec0-74588c0be509",
   "metadata": {},
   "source": [
    "The introduction of Social Health Authority(SHA) as a replacement of National Health Insurance Fund(NHIF) has caused confusion among  Kenyans due to a lack of clear,timely  and accessible information. This has led to misinformation, low enrollment and difficulties in registration and claims. \n",
    "As a result, customer service centers are overwhelmed, healthcare access is delayed, and trust in the new scheme is declining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95315cb5-14eb-4383-b068-d9001e88e945",
   "metadata": {},
   "source": [
    "#### Problem Statement \n",
    "Kenya is undergoing a major shift in its healthcare insurance system as the government pushes for full implementation of SHA. However, challenges such as misinformation, registration issues, and policy misunderstandings are slowing adoption. Proactively addressing these issues is key to ensuring a seamless transition for millions who depend on health insurance.\n",
    "\n",
    "This project aims to close the information gap on SHA by providing accurate, timely, and clear details on healthcare coverage. Through technology, public engagement, and stakeholder collaboration, it will support a smooth transition from NHIF to SHA, enhancing healthcare access and efficiency in Kenya. m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31887e9f-953e-4f5d-a7c3-97e6e7bbcf61",
   "metadata": {},
   "source": [
    "#### Main Objective \n",
    "Develop an AI Chatbot aimed at promoting understanding and increased adoption of SHA in the country by providing accurate, instant and accessible information about the scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b43c8-8b7a-4b83-b29a-38e0da5aadf5",
   "metadata": {},
   "source": [
    "#### Specific Objective \n",
    "- Implement an NLP model to understand about the scheme eligibility, benefits, contributions, claim procedures and the enrollment process. \n",
    "- Implement and deploy a web-based chatbot using Flask to ensure user interactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0d8a9",
   "metadata": {},
   "source": [
    "#### Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ee92a",
   "metadata": {},
   "source": [
    "- How can an NLP model accurately determine a user's eligibility for SHA based on provided details?\n",
    "- Can an NLP model effectively summarize SHA’s benefits based on user queries?\n",
    "- Can the chatbot validate and guide users through claim submission based on SHA’s policies?\n",
    "- Can the chatbot pre-fill application forms based on extracted user input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e6b8f",
   "metadata": {},
   "source": [
    "#### Data Understanding\n",
    "The dataset for the SHA Medical chatbot was sourced from the official SHA Website [Social Health Insurance (General) Regulations, 2023](https://www.health.go.ke/sites/default/files/2023-11/SOCIAL%20HEALTH%20INSURANCE%20%28GENERAL%29%20REGULATIONS%20%2C2023.pdf)\n",
    "\n",
    "The Intents, Domain and Stories have been developed based on the SHA Act(2023) to guarantee precise responses concerning scheme elegibility, benefits, contributions, claim procedures and enrollment process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed50a7",
   "metadata": {},
   "source": [
    "##### - Question and Answer Dataset\n",
    "\n",
    "The Question and Answer dataset for the SHA medical chatbot project contains 7,904 entries, sourced from the SHA website. It includes fully populated Question and Answer columns, capturing user inquiries and responses about the Social Health Authority scheme\n",
    "\n",
    "This dataset serves as a core resource for training the NLP model to address SHA-related topics like eligibility, benefits, and enrollment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32748b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>What is the purpose of the Primary Healthcare ...</td>\n",
       "      <td>To purchase primary healthcare services, pay h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Who is eligible to access healthcare under the...</td>\n",
       "      <td>Every person resident in Kenya who is register...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>What services are covered under the Primary He...</td>\n",
       "      <td>Promotive, preventive, curative, rehabilitativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>What is the role of the Authority in financing...</td>\n",
       "      <td>To mobilize resources for purchasing primary h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>What is the Social Health Insurance Fund used ...</td>\n",
       "      <td>To pool all contributions and purchase healthc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0         0.0   \n",
       "1             1         1.0   \n",
       "2             2         2.0   \n",
       "3             3         3.0   \n",
       "4             4         4.0   \n",
       "\n",
       "                                            Question  \\\n",
       "0  What is the purpose of the Primary Healthcare ...   \n",
       "1  Who is eligible to access healthcare under the...   \n",
       "2  What services are covered under the Primary He...   \n",
       "3  What is the role of the Authority in financing...   \n",
       "4  What is the Social Health Insurance Fund used ...   \n",
       "\n",
       "                                              Answer  \n",
       "0  To purchase primary healthcare services, pay h...  \n",
       "1  Every person resident in Kenya who is register...  \n",
       "2  Promotive, preventive, curative, rehabilitativ...  \n",
       "3  To mobilize resources for purchasing primary h...  \n",
       "4  To pool all contributions and purchase healthc...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv( 'sha_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48165c96",
   "metadata": {},
   "source": [
    "##### - Frequently Asked Questions(FAQ) Dataset\n",
    "\n",
    "A collection of common questions and answers about the Social Health Authority (SHA), sourced from official materials. It provides clear, user-friendly details on the scheme, supporting the chatbot’s NLP training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560453a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOCIAL HEALTH INSURANCE ACT 2023 &\\nSOCIAL HEALTH INSURANCE REGULATIONS 2024\\nFREQUENTLY ASKED QUESTIONS\\nImportant areas to understand:\\nA. Understanding Social Health\\nAuthority (SHA).\\nB. Institutions created by the Universal\\nHealth Coverage laws and transition\\nprocess.\\nC. NHIF staff considerations during the\\ntransition process.\\nD. Primary Health Care & the PHC fund.\\nE. Emergency, chronic and critical illness\\nfund.\\nF. Registration, means testing &\\ncontributions.\\nG. Benefits, tarrifs & claims\\nmanage'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FAQ's Dataset \n",
    "\n",
    "import os\n",
    "os.path.abspath(\"data/Frequently-Asked-Questions-FAQs-on-Social-Health-Authority-SHA-.pdf\")\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "file_path = r\"C:\\Users\\User\\Documents\\Flatiron\\CapstoneProject\\capstone_project_group2-\\data\\Frequently-Asked-Questions-FAQs-on-Social-Health-Authority-SHA-.pdf\"\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    sha_act = \"\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "#Preview the first 500 characaters \n",
    "sha_act[:500]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8891664",
   "metadata": {},
   "source": [
    "#### -Social Health Insurance(SHA) Act Dataset\n",
    "\n",
    "This dataset has been extracted from the Social Health Insurance (General) Regulations, 2023. \n",
    "\n",
    "It is based on the official legal framework of SHA in Kenya. Sourced from the Ministry of Health, it covers eligibility, benefits, contributions, claims, and enrollment, forming the chatbot’s core knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92151718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOCIAL HEALTH INSURANCE ACT 2023 &\\nSOCIAL HEALTH INSURANCE REGULATIONS 2024\\nFREQUENTLY ASKED QUESTIONS\\nImportant areas to understand:\\nA. Understanding Social Health\\nAuthority (SHA).\\nB. Institutions created by the Universal\\nHealth Coverage laws and transition\\nprocess.\\nC. NHIF staff considerations during the\\ntransition process.\\nD. Primary Health Care & the PHC fund.\\nE. Emergency, chronic and critical illness\\nfund.\\nF. Registration, means testing &\\ncontributions.\\nG. Benefits, tarrifs & claims\\nmanage'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SHA Act Dataset\n",
    "import os\n",
    "\n",
    "pdf_path = r\"data/SOCIAL HEALTH INSURANCE (GENERAL) REGULATIONS, 2023.pdf\"\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "file_path = r\"C:\\Users\\User\\Documents\\Flatiron\\CapstoneProject\\capstone_project_group2-\\data\\Frequently-Asked-Questions-FAQs-on-Social-Health-Authority-SHA-.pdf\"\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    sha_act = \"\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "#Preview the first 500 characaters \n",
    "sha_act[:500]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be328e5",
   "metadata": {},
   "source": [
    "#### Data Limitations \n",
    "- SHA Act (2023) lacks real-world user scenarios for training intents and stories.\n",
    "- PDF format of regulations hinders easy NLP parsing.\n",
    "- No historical NHIF user query data to compare with SHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b15fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9449c92e",
   "metadata": {},
   "source": [
    "The essential libraries and functions utilized in this project were consolidated in a separate notebook named My_functions, located within the project’s directory.\n",
    "\n",
    "This My_functions notebook was subsequently imported into the current notebook, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52617d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import My_functions as myf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eec4a",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80468e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7904 entries, 0 to 7903\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0.1  7904 non-null   int64  \n",
      " 1   Unnamed: 0    932 non-null    float64\n",
      " 2   Question      7904 non-null   object \n",
      " 3   Answer        7904 non-null   object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 247.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0cd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8db6079b",
   "metadata": {},
   "source": [
    "#### Summary Of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df52b3",
   "metadata": {},
   "source": [
    "The dataset consists of 7,904 entries and 4 columns. The first column serves as an index-like column with no missing entries.\n",
    "column 0 is largely incomplete, with only 932 non-null values.\n",
    "Questioncolumn is fully populated with text-based questions.\n",
    "Answer column is also fully populated with text-based responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f397b88",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273e96b",
   "metadata": {},
   "source": [
    "Imports relevant libraries that will be used to clean and process text data, then train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7ed408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "214eb036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but recommended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f98240",
   "metadata": {},
   "source": [
    "The DataPreprocessor is designed to clean and preprocess text data in a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8ae0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, df, text_columns):\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        \"\"\"Initialize with dataframe and text columns to process.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        # Drop unnecessary index-like columns, ignoring errors if they don't exist\n",
    "        self.df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], errors='ignore', inplace=True)\n",
    "        # Store the columns to preprocess\n",
    "        self.text_columns = text_columns\n",
    "        # Load English stopwords from NLTK\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Initialize NLTK's WordNet lemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def normalize_case(self, text):\n",
    "        \"\"\"Convert text to lowercase.\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"Remove punctuation from text.\"\"\"\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize the text.\"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def convert_non_alphabetic(self, tokens):\n",
    "        \"\"\"Remove non-alphabetic tokens.\"\"\"\n",
    "        return [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Apply lemmatization to tokens.\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def filter_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from token list.\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Apply all preprocessing steps to a given text.\"\"\"\n",
    "        #Convert to lowercase\n",
    "        text = self.normalize_case(text)\n",
    "        #Remove punctuation\n",
    "        text = self.remove_punctuation(text)\n",
    "        #Split into tokens\n",
    "        tokens = self.tokenize(text)\n",
    "        #Keep only alphabetic tokens\n",
    "        tokens = self.convert_non_alphabetic(tokens)\n",
    "        #Remove stopwords\n",
    "        tokens = self.filter_stopwords(tokens)\n",
    "        #Lemmatize tokens\n",
    "        tokens = self.lemmatize_tokens(tokens)\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def apply_preprocessing(self):\n",
    "        \"\"\"Apply preprocessing to all specified text columns.\"\"\"\n",
    "        # Iterate over each text column\n",
    "            # Convert to string and apply preprocessing to every entry in the column\n",
    "        for col in self.text_columns:\n",
    "            self.df[col] = self.df[col].astype(str).apply(self.preprocess_text)\n",
    "            # Return the fully processed DataFrame\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "368b840d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>purpose primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eligible access healthcare primary healthcare ...</td>\n",
       "      <td>every person resident kenya registered member ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service covered primary healthcare fund</td>\n",
       "      <td>promotive preventive curative rehabilitative p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>role authority financing primary healthcare fund</td>\n",
       "      <td>mobilize resource purchasing primary healthcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social health insurance fund used</td>\n",
       "      <td>pool contribution purchase healthcare service ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                    purpose primary healthcare fund   \n",
       "1  eligible access healthcare primary healthcare ...   \n",
       "2            service covered primary healthcare fund   \n",
       "3   role authority financing primary healthcare fund   \n",
       "4                  social health insurance fund used   \n",
       "\n",
       "                                              Answer  \n",
       "0  purchase primary healthcare service pay health...  \n",
       "1  every person resident kenya registered member ...  \n",
       "2  promotive preventive curative rehabilitative p...  \n",
       "3  mobilize resource purchasing primary healthcar...  \n",
       "4  pool contribution purchase healthcare service ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the DataPreprocessor class with the original DataFrame and specify text columns to clean\n",
    "# Then apply the preprocessing pipeline to the 'Question' and 'Answer' columns\n",
    "df_cleaned = DataPreprocessor(df, ['Question', 'Answer']).apply_preprocessing()\n",
    "\n",
    "# Display the first 5 rows of the cleaned DataFrame to inspect the preprocessed results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed8fb863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>explain appeal process denied claim</td>\n",
       "      <td>healthcare provider appeal claim denial disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>would define appeal process denied claim</td>\n",
       "      <td>healthcare provider appeal claim denial disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7886</th>\n",
       "      <td>could clarify appeal process denied claim</td>\n",
       "      <td>healthcare provider appeal claim denial disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7887</th>\n",
       "      <td>simple term appeal process denied claim</td>\n",
       "      <td>healthcare provider appeal claim denial disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>appeal process denied claim</td>\n",
       "      <td>healthcare provider appeal claim denial disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>healthcare cost controlled within scheme</td>\n",
       "      <td>tariff reviewed periodically align economic he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7890</th>\n",
       "      <td>right contributor scheme</td>\n",
       "      <td>contributor right quality healthcare transpare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>scheme integrate private health insurance</td>\n",
       "      <td>private insurer complement coverage social hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>healthcare provider charge additional fee bene...</td>\n",
       "      <td>must adhere prescribed tariff without unauthor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>scheme ensure equitable access healthcare</td>\n",
       "      <td>subsidizing contribution indigent household en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>role primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>primary healthcare fund created</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7896</th>\n",
       "      <td>primary healthcare fund function</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>objective primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7898</th>\n",
       "      <td>mandate primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7899</th>\n",
       "      <td>primary healthcare fund aim achieve</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7900</th>\n",
       "      <td>function primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7901</th>\n",
       "      <td>primary healthcare fund important</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7902</th>\n",
       "      <td>primary healthcare fund support healthcare ser...</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7903</th>\n",
       "      <td>responsibility primary healthcare fund</td>\n",
       "      <td>purchase primary healthcare service pay health...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "7884                explain appeal process denied claim   \n",
       "7885           would define appeal process denied claim   \n",
       "7886          could clarify appeal process denied claim   \n",
       "7887            simple term appeal process denied claim   \n",
       "7888                        appeal process denied claim   \n",
       "7889           healthcare cost controlled within scheme   \n",
       "7890                           right contributor scheme   \n",
       "7891          scheme integrate private health insurance   \n",
       "7892  healthcare provider charge additional fee bene...   \n",
       "7893          scheme ensure equitable access healthcare   \n",
       "7894                       role primary healthcare fund   \n",
       "7895                    primary healthcare fund created   \n",
       "7896                   primary healthcare fund function   \n",
       "7897                  objective primary healthcare fund   \n",
       "7898                    mandate primary healthcare fund   \n",
       "7899                primary healthcare fund aim achieve   \n",
       "7900                   function primary healthcare fund   \n",
       "7901                  primary healthcare fund important   \n",
       "7902  primary healthcare fund support healthcare ser...   \n",
       "7903             responsibility primary healthcare fund   \n",
       "\n",
       "                                                 Answer  \n",
       "7884  healthcare provider appeal claim denial disput...  \n",
       "7885  healthcare provider appeal claim denial disput...  \n",
       "7886  healthcare provider appeal claim denial disput...  \n",
       "7887  healthcare provider appeal claim denial disput...  \n",
       "7888  healthcare provider appeal claim denial disput...  \n",
       "7889  tariff reviewed periodically align economic he...  \n",
       "7890  contributor right quality healthcare transpare...  \n",
       "7891  private insurer complement coverage social hea...  \n",
       "7892  must adhere prescribed tariff without unauthor...  \n",
       "7893  subsidizing contribution indigent household en...  \n",
       "7894  purchase primary healthcare service pay health...  \n",
       "7895  purchase primary healthcare service pay health...  \n",
       "7896  purchase primary healthcare service pay health...  \n",
       "7897  purchase primary healthcare service pay health...  \n",
       "7898  purchase primary healthcare service pay health...  \n",
       "7899  purchase primary healthcare service pay health...  \n",
       "7900  purchase primary healthcare service pay health...  \n",
       "7901  purchase primary healthcare service pay health...  \n",
       "7902  purchase primary healthcare service pay health...  \n",
       "7903  purchase primary healthcare service pay health...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last 20 rows of the cleaned DataFrame to inspect the preprocessed results\n",
    "df_cleaned.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "523f0c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the total number of duplicate rows in the cleaned DataFrame\n",
    "df_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed2bc595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the duplicates\n",
    "df_cleaned=df_cleaned.drop_duplicates(keep=\"first\")\n",
    "df_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc704910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned DataFrame to a CSV file named 'cleaned_sha_data.csv' for storage or further use\n",
    "df_cleaned.to_csv(\"cleaned_sha_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da6da1",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051e043",
   "metadata": {},
   "source": [
    "####  XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357d06d",
   "metadata": {},
   "source": [
    "Import the necessary libraries for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d80f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a29810",
   "metadata": {},
   "source": [
    "Preprocesses text by converting it to lowercase, removing punctuation, tokenizing it into words using NLTK and filtering out stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0b4e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join([word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4431596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "stop_words = set(stopwords.words('english'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecf176a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         purpose primary healthcare fund\n",
       "1       eligible access healthcare primary healthcare ...\n",
       "2                services covered primary healthcare fund\n",
       "3        role authority financing primary healthcare fund\n",
       "4                       social health insurance fund used\n",
       "                              ...                        \n",
       "7899                  primary healthcare fund aim achieve\n",
       "7900                     function primary healthcare fund\n",
       "7901                    primary healthcare fund important\n",
       "7902    primary healthcare fund support healthcare ser...\n",
       "7903             responsibilities primary healthcare fund\n",
       "Name: Processed_Question, Length: 7904, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Processed_Question\"] = df[\"Question\"].apply(preprocess_text)\n",
    "df[\"Processed_Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be026631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Transform the 'Processed_Question' column into a TF-IDF matrix\n",
    "X = vectorizer.fit_transform(df[\"Processed_Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdbb7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Answer' column to categorical type and encode categories as numeric values\n",
    "df[\"Answer_Label\"] = df[\"Answer\"].astype('category').cat.codes\n",
    "# Assign the encoded labels to 'y' as the target variable\n",
    "y = df[\"Answer_Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03ada8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f28d3cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=549, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=549, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=549, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize XGBoost classifier for multi-class classification\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(df[\"Answer\"].unique()), eval_metric=\"mlogloss\")\n",
    "\n",
    "# Train the classifier on the training dataset\n",
    "xgb_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6139f5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([261, 163, 412, ..., 311, 355, 149])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the class labels for the test dataset using the trained XGBoost model\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c130af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model by comparing predicted labels with actual labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's accuracy, formatted to two decimal places\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f53fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: To purchase primary healthcare services, pay health facilities, and establish a pool for receipt and payment of funds.\n"
     ]
    }
   ],
   "source": [
    "def get_answer(user_query):\n",
    "    # Preprocess the input question\n",
    "    user_query = preprocess_text(user_query)\n",
    "    user_vector = vectorizer.transform([user_query])\n",
    "    \n",
    "    # Predict the best answer category\n",
    "    predicted_label = xgb_classifier.predict(user_vector)[0]\n",
    "    \n",
    "    # Retrieve the corresponding answer\n",
    "    answer = df[df[\"Answer_Label\"] == predicted_label][\"Answer\"].values[0]\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What is the purpose of the primary healthcare fund?\"\n",
    "response = get_answer(user_query)\n",
    "print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a81e4679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Purchase primary healthcare services from primary healthcare facilities. Pay health facilities for providing quality primary healthcare services. Establish a pool for receipt and payment of funds for primary healthcare.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"The main purpose of the healthcare fund?\"\n",
    "response = get_answer(user_query)\n",
    "print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4c377",
   "metadata": {},
   "source": [
    "The model achieves a strong accuracy of 89%, delivering correct responses when the question matches one in the dataset. However, when the question is rephrased in a way that is not present in the dataset, the response may not be as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137b79",
   "metadata": {},
   "source": [
    "####  Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "591c994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForQuestionAnswering, RobertaTokenizer, AdamW\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c268ef",
   "metadata": {},
   "source": [
    "- This code defines a BERT-based Question Answering model using RoBERTa fine-tuned on SQuAD2. It also incorporates SentenceTransformer for retrieving the most relevant context from a dataset before generating an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3fe32e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\phase5_new\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\User\\.conda\\envs\\phase5_new\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\.conda\\envs\\phase5_new\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Answer:  if they are registered and contribute to the Social Health Insurance Fund (Confidence: 1.10)\n"
     ]
    }
   ],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataframe, model_name='all-mpnet-base-v2'):\n",
    "        self.questions = dataframe['Question'].tolist()\n",
    "        self.answers = dataframe['Answer'].tolist()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question_embedding = self.model.encode(self.questions[idx], convert_to_tensor=True)\n",
    "        answer_embedding = self.model.encode(self.answers[idx], convert_to_tensor=True)\n",
    "        \n",
    "        return {\n",
    "            'question_embedding': question_embedding,\n",
    "            'answer_embedding': answer_embedding,\n",
    "        }\n",
    "class BERTQuestionAnswering:\n",
    "    def __init__(self, model_name='deepset/roberta-base-squad2', lr=3e-5):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.model = RobertaForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.embedding_model = SentenceTransformer('all-mpnet-base-v2')  # Improved embedding model\n",
    "\n",
    "    def find_relevant_context(self, question, dataframe, threshold=0.5, top_n=3):\n",
    "        contexts = dataframe['Answer'].tolist()\n",
    "        question_embedding = self.embedding_model.encode([question], convert_to_tensor=False)\n",
    "        context_embeddings = self.embedding_model.encode(contexts, convert_to_tensor=False)\n",
    "        similarities = cosine_similarity(question_embedding, context_embeddings)[0]\n",
    "        \n",
    "        best_indices = similarities.argsort()[-top_n:][::-1]  # Get top N most similar\n",
    "        best_similarities = [similarities[i] for i in best_indices]\n",
    "        \n",
    "        if max(best_similarities) < threshold:\n",
    "            return \"No relevant context found.\"\n",
    "        \n",
    "        best_contexts = \" \".join([contexts[i] for i in best_indices])\n",
    "        return best_contexts\n",
    "    \n",
    "    def answer_question(self, question, dataframe):\n",
    "        context = self.find_relevant_context(question, dataframe)\n",
    "        if context == \"No relevant context found.\":\n",
    "            return context\n",
    "        \n",
    "        inputs = self.tokenizer(question, context, return_tensors='pt', truncation=True, padding='longest', max_length=384).to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        answer_start = torch.argmax(outputs.start_logits).item()\n",
    "        answer_end = torch.argmax(outputs.end_logits).item() + 1\n",
    "        \n",
    "        if answer_start >= answer_end or (answer_end - answer_start) > 30:\n",
    "            return \"Sorry, I couldn't find a relevant answer.\"\n",
    "        \n",
    "        answer = self.tokenizer.convert_tokens_to_string(\n",
    "            self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "        )\n",
    "        \n",
    "        answer_start_score = torch.max(outputs.start_logits).item()\n",
    "        answer_end_score = torch.max(outputs.end_logits).item()\n",
    "        confidence = (answer_start_score + answer_end_score) / 2  # Average confidence\n",
    "        \n",
    "        return f\"Answer: {answer} (Confidence: {confidence:.2f})\"\n",
    "\n",
    "# Load dataset\n",
    "dataframe = pd.read_csv(\"sha_dataset.csv\").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = BERTQuestionAnswering()\n",
    "\n",
    "# Ask a question without specifying context\n",
    "question = \"Who is eligible to access healthcare?\"\n",
    "answer = model.answer_question(question, dataframe)\n",
    "\n",
    "print(\"Predicted Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "609545d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Predicted Answer: Answer:  provides essential healthcare services to eligible individuals (Confidence: 6.08)\n"
     ]
    }
   ],
   "source": [
    "# Ask a question without specifying context\n",
    "question = \"What is the role of the Primary Healthcare Fund??\"\n",
    "answer = model.answer_question(question, dataframe)\n",
    "\n",
    "print(\"\\n✅ Predicted Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b6d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
